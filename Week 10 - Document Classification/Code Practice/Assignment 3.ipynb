{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification using Naive Bayse Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the 20 Newsgroups Data and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 2034\n",
      "#Test set size: 1353\n",
      "#Selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "#Train labels: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Create a list of categories to select from the 20 topics\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Fetch the training dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "# Remove parts of the email content that provide hints - classify purely based on content\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "# Fetch the validation dataset\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "print('#Train set size:', len(newsgroups_train.data))\n",
    "print('#Test set size:', len(newsgroups_test.data))\n",
    "print('#Selected categories:', newsgroups_train.target_names)\n",
    "print('#Train labels:', set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set text samples: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "#Train set label smaples: 1\n",
      "#Test set text samples: TRry the SKywatch project in  Arizona.\n",
      "#Test set label smaples: 2\n"
     ]
    }
   ],
   "source": [
    "print('#Train set text samples:', newsgroups_train.data[0])\n",
    "print('#Train set label smaples:', newsgroups_train.target[0])\n",
    "print('#Test set text samples:', newsgroups_test.data[0])\n",
    "print('#Test set label smaples:', newsgroups_test.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (2034, 2000)\n",
      "Test set dimension: (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train = newsgroups_train.data   # Training dataset documents\n",
    "y_train = newsgroups_train.target  # Training dataset labels\n",
    "\n",
    "X_test = newsgroups_test.data      # Validation dataset documents\n",
    "y_test = newsgroups_test.target     # Validation dataset labels\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)  # Transform the training set\n",
    "print('Train set dimension:', X_train_cv.shape) \n",
    "X_test_cv = cv.transform(X_test)  # Transform the test set\n",
    "print('Test set dimension:', X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0, 000 : 0, 01 : 0, 04 : 0, 05 : 0, 10 : 0, 100 : 0, 1000 : 0, 11 : 0, 12 : 0, 128 : 0, 129 : 0, 13 : 0, 130 : 0, 14 : 0, 15 : 0, 16 : 0, 17 : 0, 18 : 0, 19 : 0, 1987 : 0, 1988 : 0, 1989 : 0, 1990 : 0, 1991 : 0, 1992 : 0, 1993 : 0, 20 : 0, 200 : 0, 202 : 0, 21 : 0, 22 : 0, 23 : 0, 24 : 0, 25 : 0, 256 : 0, 26 : 0, 27 : 0, 28 : 0, 2d : 0, 30 : 0, 300 : 0, 31 : 0, 32 : 0, 33 : 0, 34 : 0, 35 : 0, 39 : 0, 3d : 0, 40 : 0, 400 : 0, 42 : 0, 45 : 0, 50 : 0, 500 : 0, 60 : 0, 600 : 0, 65 : 0, 70 : 0, 75 : 0, 80 : 0, 800 : 0, 90 : 0, 900 : 0, 91 : 0, 92 : 0, 93 : 0, 95 : 0, _the : 0, ability : 0, able : 1, abortion : 0, about : 1, above : 0, absolute : 0, absolutely : 0, ac : 0, accept : 0, acceptable : 0, accepted : 0, access : 0, according : 0, account : 0, accurate : 0, across : 0, act : 0, action : 0, actions : 0, active : 0, activities : 0, activity : 0, acts : 0, actual : 0, actually : 0, ad : 0, add : 0, added : 0, addition : 0, additional : 0, address : 0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0, :100]):\n",
    "    print(word, ':', count, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Classification using k-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.739\n",
      "Test set score: 0.387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  # Import KNN classifier\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=2)  # You can choose the number of neighbors\n",
    "\n",
    "KNN_clf.fit(X_train_cv, y_train)  # Train the classifier using the CountVectorizer training set\n",
    "\n",
    "print('Train set score: {:.3f}'.format(KNN_clf.score(X_train_cv, y_train)))  # Check prediction accuracy on the training set\n",
    "print('Test set score: {:.3f}'.format(KNN_clf.score(X_test_cv, y_test)))  # Check prediction accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#First document and label in test data: TRry the SKywatch project in  Arizona. 2\n",
      "#Second document and label in test data: The Vatican library recently made a tour of the US.\n",
      " Can anyone help me in finding a FTP site where this collection is \n",
      " available. 1\n",
      "#Predicted labels: [1 3]\n",
      "#Predicted categories: comp.graphics talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print('#First document and label in test data:', X_test[0], y_test[0])\n",
    "print('#Second document and label in test data:', X_test[1], y_test[1])\n",
    "\n",
    "pred = KNN_clf.predict(X_test_cv[:2])  # Predictions for the test set using CountVectorizer\n",
    "\n",
    "print('#Predicted labels:', pred)\n",
    "print('#Predicted categories:', newsgroups_train.target_names[pred[0]], newsgroups_train.target_names[pred[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.830\n",
      "Test set score: 0.336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use the same arguments as CountVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5) \n",
    "X_train_tfidf = tfidf.fit_transform(X_train)  # Transform the training set\n",
    "X_test_tfidf = tfidf.transform(X_test)  # Transform the test set\n",
    "\n",
    "KNN_clf.fit(X_train_tfidf, y_train)  # Train the classifier using the TF-IDF training set\n",
    "print('Train set score: {:.3f}'.format(KNN_clf.score(X_train_tfidf, y_train)))  # Check prediction accuracy on the training set\n",
    "print('Test set score: {:.3f}'.format(KNN_clf.score(X_test_tfidf, y_test)))  # Check prediction accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: image, orbit, 3d, file, thanks, you, nasa, god, graphics, space\n",
      "comp.graphics: image, orbit, 3d, file, thanks, you, nasa, god, graphics, space\n",
      "sci.space: image, orbit, 3d, file, thanks, you, nasa, god, graphics, space\n",
      "talk.religion.misc: image, orbit, 3d, file, thanks, you, nasa, god, graphics, space\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier로 모델 재학습\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(tfidf.transform(newsgroups_train.data), newsgroups_train.target)\n",
    "\n",
    "def top10_features(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        # Feature importance를 기반으로 정렬\n",
    "        top10 = np.argsort(classifier.feature_importances_)[-10:]  # 상위 10개 특성\n",
    "        # Print the category and the top 10 influential features\n",
    "        print(\"%s: %s\" % (category, \", \".join(feature_names[top10])))\n",
    "\n",
    "top10_features(rf_clf, tfidf, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minjoo",
   "language": "python",
   "name": "iitpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
