{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uirjKH1_0Fa"
      },
      "source": [
        "# Week 12 - Sentiment Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed_9Lvoa_0Fe"
      },
      "source": [
        "## Sentiment Analysis Using Sentiment Lexicons\n",
        "\n",
        "### Preparing the NLTK Movie Review Dataset\n",
        "\n",
        "https://www.nltk.org/book/ch02.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQp6XTOz_0Ff",
        "outputId": "2bef7d79-fd2a-4822-ce9d-939e9efc9a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#review count: 2000\n",
            "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
            "#categories of reviews: ['neg', 'pos']\n",
            "#num of \"neg\" reviews: 1000\n",
            "#num of \"pos\" reviews: 1000\n",
            "#id of the first review: neg/cv000_29416.txt\n",
            "#part of the first review: plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
            "what's the deal ? \n",
            "watch the movie and \" sorta \" find out . . . \n",
            "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
            "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
            "#sentiment of the first review: ['neg']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "print('#review count:', len(movie_reviews.fileids()))  # Returns the IDs of the movie review documents\n",
        "print('#samples of file ids:', movie_reviews.fileids()[:10])  # Prints the first 10 file IDs\n",
        "print('#categories of reviews:', movie_reviews.categories())  # Returns the categories, i.e., whether positive or negative\n",
        "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg')))  # Returns the count of reviews labeled as negative\n",
        "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos')))  # Returns the count of reviews labeled as positive\n",
        "\n",
        "fileid = movie_reviews.fileids()[0]  # Returns the ID of the first document\n",
        "print('#id of the first review:', fileid)\n",
        "print('#part of the first review:', movie_reviews.raw(fileid)[:500])  # Prints the first 500 characters of the first review\n",
        "print('#sentiment of the first review:', movie_reviews.categories(fileid))  # Prints the sentiment (category) of the first review\n",
        "\n",
        "fileids = movie_reviews.fileids()  # Retrieves the file IDs from the movie review data\n",
        "reviews = [movie_reviews.raw(fileid) for fileid in fileids]  # Retrieves the raw text files using the file IDs\n",
        "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]  # Retrieves the category (sentiment) for each review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekbni3Gu_0Fi"
      },
      "source": [
        "### Sentiment Analysis Using TextBlob\n",
        "\n",
        "https://textblob.readthedocs.io/en/dev/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MMufCqC_0Fj"
      },
      "source": [
        "https://textblob.readthedocs.io/en/dev/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1UJ-aQo_0Fj",
        "outputId": "ed99e6c3-e6ca-4fbc-854e-eeddf05b95d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (4.66.6)\n",
            "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.17.1\n",
            "    Uninstalling textblob-0.17.1:\n",
            "      Successfully uninstalled textblob-0.17.1\n",
            "Successfully installed textblob-0.18.0.post0\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbTTpBF1_0Fk",
        "outputId": "e0e6c2f2-2f29-45f3-cf29-a1d8400ca2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.06479782948532947, subjectivity=0.5188408350908352)\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "result = TextBlob(reviews[0])\n",
        "print(result.sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IodrNP5i_0Fk"
      },
      "outputs": [],
      "source": [
        "def sentiment_TextBlob(docs):\n",
        "    results = []\n",
        "\n",
        "    for doc in docs:\n",
        "        testimonial = TextBlob(doc)\n",
        "        if testimonial.sentiment.polarity > 0:\n",
        "            results.append('pos')\n",
        "        else:\n",
        "            results.append('neg')\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlYV4n8j_0Fl",
        "outputId": "3bdeffec-33bf-47fb-f548-a32ff1bfd8d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Accuracy of sentiment analysis using TextBlob: 0.6\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Accuracy of sentiment analysis using TextBlob\n",
        "print('#Accuracy of sentiment analysis using TextBlob:', accuracy_score(categories, sentiment_TextBlob(reviews)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awXe3eqB_0Fl"
      },
      "source": [
        "### Sentiment Analysis Using AFINN\n",
        "\n",
        "https://github.com/fnielsen/afinn\n",
        "\n",
        "http://corpustext.com/reference/sentiment_afinn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIvLbZUr_0Fl",
        "outputId": "ed1ac01d-fc6c-46d2-b1a0-1a2ade4df099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: afinn in /usr/local/lib/python3.10/dist-packages (0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install afinn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic-zQTPp_0Fm",
        "outputId": "58a66f0a-8298-4e5b-83ec-23c8fddc42e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Accuracy of sentiment analysis using Affin: 0.664\n"
          ]
        }
      ],
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "def sentiment_Afinn(docs):\n",
        "    afn = Afinn(emoticons=True)\n",
        "    results = []\n",
        "\n",
        "    for doc in docs:\n",
        "        if afn.score(doc) > 0:\n",
        "            results.append('pos')\n",
        "        else:\n",
        "            results.append('neg')\n",
        "    return results\n",
        "\n",
        "print('#Accuracy of sentiment analysis using Affin:', accuracy_score(categories, sentiment_Afinn(reviews)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh9vtp5f_0Fm"
      },
      "source": [
        "### Sentiment Analysis Using VADER\n",
        "\n",
        "https://github.com/cjhutto/vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbuHurzd_0Fn",
        "outputId": "5a086865-37a6-4ae8-91c8-7730791c3800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qCd0j0P_0Fn",
        "outputId": "9245a067-c7fe-4a5d-8ee2-fd635ff2e818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Accuracy of sentiment analysis using VADER: 0.635\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def sentiment_vader(docs):\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    results = []\n",
        "\n",
        "    for doc in docs:\n",
        "        score = analyser.polarity_scores(doc)\n",
        "        if score['compound'] > 0:\n",
        "            results.append('pos')\n",
        "        else:\n",
        "            results.append('neg')\n",
        "\n",
        "    return results\n",
        "\n",
        "print('#Accuracy of sentiment analysis using VADER:', accuracy_score(categories, sentiment_vader(reviews)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOoe7gY1_0Fo"
      },
      "source": [
        "### Korean Sentiment Lexicon\n",
        "\n",
        "1: https://github.com/park1200656/KnuSentiLex   \n",
        "2: https://github.com/mrlee23/KoreanSentimentAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAKVSG5E_0Fo"
      },
      "source": [
        "## Sentiment Analysis Based on Machine Learning through Training\n",
        "\n",
        "### Machine Learning-Based Sentiment Analysis on NLTK Movie Reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoYf7nSX_0Fp",
        "outputId": "e8b6e07b-ed96-43cc-dd79-8667b35235db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set count:  1600\n",
            "Test set count:  400\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split  # Using the split function provided by sklearn\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=7)\n",
        "\n",
        "print('Train set count: ', len(X_train))\n",
        "print('Test set count: ', len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee16UuI__0Fq",
        "outputId": "4a04ad82-bf83-4c5e-d5e0-0a0a512846e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set dimension: (1600, 36189)\n",
            "#Test set dimension: (400, 36189)\n",
            "#Train set score: 0.998\n",
            "#Test set score: 0.797\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB  # Using the MultinomialNB provided by sklearn\n",
        "\n",
        "tfidf = TfidfVectorizer().fit(X_train)\n",
        "\n",
        "X_train_tfidf = tfidf.transform(X_train)  # Transform the train set\n",
        "print('#Train set dimension:', X_train_tfidf.shape)  # Check how many features are actually used\n",
        "X_test_tfidf = tfidf.transform(X_test)  # Transform the test set\n",
        "print('#Test set dimension:', X_test_tfidf.shape)\n",
        "\n",
        "NB_clf = MultinomialNB(alpha=0.01)  # Declare the classifier\n",
        "NB_clf.fit(X_train_tfidf, y_train)  # Train the classifier using the train set\n",
        "print('#Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))  # Check the prediction accuracy on the train set\n",
        "print('#Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))  # Check the prediction accuracy on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZABFysM_0Fs"
      },
      "source": [
        "\n",
        "| | Predicted Positive Reviews (PP) | Predicted Negative Reviews (PN) |\n",
        "|---|---|---|\n",
        "|Actual Positive Reviews (P) | True positive(TP) | False negative(FN) |\n",
        "|Actual Negative Reviews (N) | False positive(FP) | True negative(TN) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYPaQGXhEzFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string  # Add this import to resolve the error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download necessary resources for nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load IMDB dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=1000)  # Use the top 10,000 most frequent words\n",
        "\n",
        "# Function to decode the text data from word indices to actual words\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def decode_review(encoded_review):\n",
        "    reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
        "\n",
        "# Decode IMDB reviews into actual text from word indices\n",
        "X_train_text = [decode_review(review) for review in X_train]\n",
        "X_test_text = [decode_review(review) for review in X_test]\n",
        "\n",
        "# Text preprocessing (convert to lowercase and remove punctuation)\n",
        "def text_preprocessor(text):\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])  # Use string.punctuation\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer(tokenizer=text_preprocessor, max_features=2000, min_df=5, max_df=0.5)\n",
        "\n",
        "# Transform training and test data using TF-IDF\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf.transform(X_test_text)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "LR_clf = LogisticRegression(max_iter=1000)\n",
        "LR_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate performance on training and test sets\n",
        "y_train_predict = LR_clf.predict(X_train_tfidf)\n",
        "y_test_predict = LR_clf.predict(X_test_tfidf)\n",
        "\n",
        "# Training set performance\n",
        "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train, y_train_predict)))\n",
        "print('#Precision for train set: {:.3f}'.format(precision_score(y_train, y_train_predict)))\n",
        "print('#Recall for train set: {:.3f}'.format(recall_score(y_train, y_train_predict)))\n",
        "print('#F1 for train set: {:.3f}'.format(f1_score(y_train, y_train_predict)))\n",
        "\n",
        "# Test set performance\n",
        "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test, y_test_predict)))\n",
        "print('#Precision for test set: {:.3f}'.format(precision_score(y_test, y_test_predict)))\n",
        "print('#Recall for test set: {:.3f}'.format(recall_score(y_test, y_test_predict)))\n",
        "print('#F1 for test set: {:.3f}'.format(f1_score(y_test, y_test_predict)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51B2fRJOE0ES",
        "outputId": "ddb66644-8885-415d-a1b5-90a2bce359e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Accuracy for train set: 0.899\n",
            "#Precision for train set: 0.893\n",
            "#Recall for train set: 0.907\n",
            "#F1 for train set: 0.900\n",
            "#Accuracy for test set: 0.877\n",
            "#Precision for test set: 0.872\n",
            "#Recall for test set: 0.884\n",
            "#F1 for test set: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38tRj0bGFWtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}