{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xxj5-oaolEz",
        "outputId": "ff6d5cc5-9711-44f8-b376-12c3f81f1a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import string  # Add this import to resolve the error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download necessary resources for nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=1000)\n",
        "# This loads the IMDB dataset from Keras. It is preprocessed to contain reviews represented as word indices\n",
        "# instead of raw text.\n",
        "# - X_train, X_test: These are the reviews in the dataset, encoded as sequences of integers (word indices).\n",
        "# - y_train, y_test: These are the corresponding labels (0 = negative review, 1 = positive review).\n",
        "# - num_words=1000: This limits the dataset to the top 1000 most frequently occurring words in the entire dataset.\n",
        "#   This simplifies the model by focusing on commonly used words while ignoring rare ones.\n",
        "\n",
        "# Function to decode the text data from word indices to actual words\n",
        "word_index = imdb.get_word_index()\n",
        "# `word_index` is a dictionary mapping words to their corresponding integer indices in the IMDB dataset.\n",
        "# This is required to convert encoded reviews (sequences of integers) back into human-readable text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1p33Us3o-6E",
        "outputId": "cc5a1fc3-2630-4b63-f3ea-72c612061be4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_review(encoded_review):\n",
        "    # Create a reverse mapping from integer indices back to words\n",
        "    reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "    # Decode the encoded review:\n",
        "    # - Subtract 3 from each index because Keras reserves indices 0, 1, and 2 for special purposes.\n",
        "    # - Replace indices not found in `reverse_word_index` with a placeholder ('?') for missing words.\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])"
      ],
      "metadata": {
        "id": "sW6L1RmLorpb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode IMDB reviews into actual text from word indices\n",
        "X_train_text = [decode_review(review) for review in X_train]\n",
        "# Convert each encoded review in the training set (X_train) into human-readable text using the `decode_review` function.\n",
        "# This creates a list of text reviews corresponding to the encoded data.\n",
        "\n",
        "X_test_text = [decode_review(review) for review in X_test]\n",
        "# Similarly, convert each encoded review in the test set (X_test) into human-readable text.\n",
        "\n",
        "# Why this decoding step is necessary:\n",
        "# - The IMDB dataset is provided in an encoded format (integer sequences representing word indices) for\n",
        "#   computational efficiency.\n",
        "# - However, human-readable text is essential for performing further preprocessing steps (e.g., removing punctuation,\n",
        "#   lowercasing) and understanding the dataset for debugging and analysis."
      ],
      "metadata": {
        "id": "ABlgr1oTorix"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing (convert to lowercase and remove punctuation)\n",
        "def text_preprocessor(text):\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])  # Use string.punctuation\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "1eQMO8I6orb-"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}